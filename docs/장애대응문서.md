# 부하 테스트 기반 장애 대응 보고서

**작성자**: 홍석호  
**작성일**: 2025-06-06  
**버전**: 1.0

## 개요

**목적**: 부하 테스트 결과 분석을 통해 예상 장애 시나리오를 도출하고 사전 대응 체계를 구축합니다.

**핵심 발견사항**: 현재 성능은 양호하나, 트래픽 급증 시 Redis 연결 풀 고갈과 Kafka 프로듀서 지연이 주요 장애 지점으로 예상됩니다.

**예상 비즈니스 임팩트**:
- 쿠폰 발급 실패 → 고객 불만 및 이벤트 신뢰도 하락
- 상품 조회 지연 → 구매 전환율 감소 (응답 시간 1초 당 7% 감소)

---

## 부하 테스트 결과 분석

### 현재 성능 수준
**선착순 쿠폰 발급**:
- p99 응답 시간: 21.1ms (목표 250ms 대비 양호)
- 1,000명 동시 사용자까지 안정적 처리
- 쿠폰 발급 성공률: 100%

**상위 상품 조회**:
- p99 응답 시간: 24.97ms (목표 250ms 대비 양호)
- 800명 동시 사용자까지 안정적 처리
- 캐시 히트율: 100%

### 잠재적 장애 지점 식별

#### 1. Redis 연결 풀 고갈 (P1 장애)
**예상 시나리오**: 트래픽이 현재 테스트 대비 3배 이상 급증 시
- 동시 연결 수 한계 초과
- 새로운 연결 요청 타임아웃 발생
- 캐시 조회 실패로 DB 직접 조회 증가

#### 2. Kafka 프로듀서 지연 (P2 장애)
**예상 시나리오**: 쿠폰 발급 요청이 초당 2,000건 초과 시
- Kafka 메시지 적재 지연
- 백프레셔로 인한 API 응답 지연
- 쿠폰 발급 상태 불일치 발생

#### 3. MySQL 커넥션 풀 고갈 (P0 장애)
**예상 시나리오**: Redis 장애로 DB 직접 조회 급증 시
- 커넥션 풀 모든 연결 사용 중
- 새로운 DB 요청 대기 큐 적재
- 전체 서비스 응답 불가

---

## 장애 시나리오별 대응 계획

### 시나리오 1: Redis 연결 풀 고갈

#### 감지 방법
**모니터링 지표**:
- Redis 연결 수 사용률 > 80%
- API 응답 시간 p99 > 500ms
- 캐시 히트율 < 90%

**알림 설정**:
- **P1 (High)**: Redis 연결 수 사용률 90% 초과 시
- **P2 (Medium)**: API 응답 시간 p99 1s 초과 시
- **P3 (Low)**: 캐시 히트율 80% 이하 시
- **알림 채널**: Slack, 이메일, SMS

#### 즉시 대응 (5분 내)
1. **Redis 연결 풀 확장**: `maxActive: 50 → 100`
2. **불필요한 Redis 연결 정리**: 유휴 연결 강제 해제
3. **DB 직접 조회 임시 활성화**: 캐시 실패 시 fallback 동작 확인

#### 단기 대응 (30분 내)
1. **Redis 인스턴스 스케일업**: t4g.micro → t4g.small
2. **연결 타임아웃 조정**: `timeout: 3s → 1s` (빠른 실패)
3. **캐시 TTL 단축**: 메모리 사용량 감소

#### 중장기 대응 (1주일 내)
1. **Redis 클러스터 구축**: 읽기 전용 복제본 3대 추가
2. **연결 풀 모니터링 강화**: 실시간 대시보드 구축
3. **Circuit Breaker 패턴 적용**: Redis 장애 시 자동 차단

### 시나리오 2: Kafka 프로듀서 지연

#### 감지 방법
**모니터링 지표**:
- Kafka 프로듀서 지연 시간 > 100ms
- 쿠폰 발급 API 응답 시간 > 1s
- Kafka 토픽 지연(lag) > 1,000건

**로그 기반 감지**:
```json
{
  "level": "ERROR",
  "message": "Kafka producer timeout",
  "duration_ms": 5000,
  "topic": "coupon-issue",
  "partition": 1
}
```

#### 즉시 대응 (10분 내)
1. **배치 사이즈 조정**: `batch.size: 1000 → 500`
2. **비동기 처리 활성화**: 쿠폰 발급을 즉시 응답 후 백그라운드 처리
3. **Kafka 파티션 추가**: 처리량 증대

#### 단기 대응 (1시간 내)
1. **Kafka 브로커 스케일업**: m7g.large → m7g.xlarge
2. **프로듀서 설정 최적화**: 
   - `acks: all → 1` (성능 우선)
   - `compression.type: none → snappy`
3. **백프레셔 임계치 설정**: 큐 사이즈 모니터링

#### 중장기 대응 (2주일 내)
1. **이벤트 소싱 패턴 도입**: 쿠폰 발급 상태 추적 개선
2. **Kafka 클러스터 확장**: 브로커 3대 → 5대
3. **메시지 압축 및 배치 처리 최적화**

### 시나리오 3: MySQL 커넥션 풀 고갈

#### 감지 방법
**모니터링 지표**:
- MySQL 활성 연결 수 > 90%
- DB 응답 시간 p99 > 2s
- API 5xx 에러율 > 5%

**알림 우선순위**: P0 (Critical) - 즉시 대응 필요

#### 즉시 대응 (2분 내)
1. **커넥션 풀 긴급 확장**: `maxActive: 20 → 50`
2. **읽기 전용 쿼리 분리**: 상품 조회를 읽기 전용 DB로 라우팅
3. **불필요한 트랜잭션 해제**: 오래 실행되는 쿼리 강제 종료

#### 단기 대응 (15분 내)
1. **DB 인스턴스 스케일업**: t4g.micro → t4g.medium
2. **쿼리 최적화**: 실행 계획 확인 후 인덱스 추가
3. **커넥션 타임아웃 단축**: `connectionTimeout: 30s → 5s`

#### 중장기 대응 (1주일 내)
1. **읽기 복제본 구축**: 조회 쿼리 부하 분산
2. **쿼리 성능 모니터링**: Slow Query 로그 분석 자동화
3. **DB 샤딩 검토**: 데이터 분산 처리 설계

---

## 장애 대응 프로세스

### MTTD/MTTR 목표
- **MTTD (Mean Time To Detection)**: 2분 이내
- **MTTR (Mean Time To Recovery)**: P0 15분, P1 30분, P2 1시간

### 대응 체계
**1단계: 감지 (Detection)**
- Datadog/Grafana 자동 알림
- Slack #alert-channel 즉시 통보
- 온콜 엔지니어 자동 호출

**2단계: 분류 (Classification)**
- 장애 레벨 분류 (P0/P1/P2)
- 영향 범위 및 고객 수 파악
- 대응 팀 구성

**3단계: 대응 (Response)**
- 표준 대응 절차 수행
- 상황 브리핑 (15분마다)
- 임시 해결책 적용

**4단계: 복구 (Recovery)**
- 근본 원인 해결
- 서비스 정상 상태 확인
- 고객 공지 및 사과

**5단계: 회고 (Retrospective)**
- 포스트모템 미팅 (장애 후 24시간 내)
- 근본 원인 분석 (5 Why)
- 재발 방지 액션 아이템 도출

---

## 모니터링 및 알림 체계

### 핵심 지표 대시보드
**시스템 메트릭**:
- CPU, Memory, Disk 사용률
- 네트워크 I/O 및 연결 수
- JVM 힙 메모리 및 GC 빈도

**애플리케이션 메트릭**:
- API 응답 시간 (p50, p95, p99)
- TPS 및 에러율
- 쿠폰 발급 성공률

**인프라 메트릭**:
- Redis 연결 수 및 메모리 사용률
- MySQL 커넥션 풀 및 슬로우 쿼리
- Kafka 지연 및 처리량

### 알림 매트릭스
| 지표 | Warning | Critical | 대응 시간 |
|------|---------|----------|-----------|
| API 응답 시간 | p99 > 500ms | p99 > 1s | 5분/2분 |
| 에러율 | > 1% | > 5% | 10분/즉시 |
| Redis 연결 | > 70% | > 90% | 15분/5분 |
| DB 연결 | > 80% | > 95% | 10분/즉시 |

---

## 5Why 근본 원인 분석

### 시나리오 1: Redis 연결 풀 고갈 장애

**1. 왜 Redis 연결 풀이 고갈되었는가?**
   - 트래픽 급증으로 동시 연결 수가 최대 한계(50개)를 초과했습니다.

**2. 왜 연결 수 한계를 초과했는가?**
   - 이벤트 시 예상 트래픽(3배 증가)에 대한 용량 계획이 부족했습니다.

**3. 왜 용량 계획이 부족했는가?**
   - 과거 이벤트 데이터 분석 없이 최소 스펙으로 설계했습니다.

**4. 왜 과거 데이터 분석이 이루어지지 않았는가?**
   - 트래픽 패턴 모니터링 체계가 구축되지 않았습니다.

**5. 왜 모니터링 체계가 구축되지 않았는가?**
   - 초기 개발 시 성능보다 기능 구현을 우선시했고, 운영 관점의 설계가 부족했습니다.

**근본 원인**: 운영 중심 설계 사고 부족 및 성능 모니터링 체계 미흡

### 시나리오 2: Kafka 프로듀서 지연 장애

**1. 왜 Kafka 프로듀서에서 지연이 발생했는가?**
   - 초당 2,000건 이상의 메시지 전송으로 브로커 처리 한계를 초과했습니다.

**2. 왜 브로커 처리 한계를 초과했는가?**
   - 단일 브로커(m7g.large) 스펙이 대량 트래픽 처리에 부족했습니다.

**3. 왜 브로커 스펙이 부족했는가?**
   - 쿠폰 발급 피크 시간대의 메시지 처리량을 정확히 예측하지 못했습니다.

**4. 왜 메시지 처리량 예측이 부정확했는가?**
   - Kafka 성능 테스트 시 실제 메시지 크기와 직렬화 오버헤드를 고려하지 않았습니다.

**5. 왜 실제 조건을 고려하지 않았는가?**
   - 성능 테스트 환경이 운영 환경과 달라 현실적 부하 테스트가 이루어지지 않았습니다.

**근본 원인**: 운영 환경과 동일한 조건의 성능 테스트 부족

### 시나리오 3: MySQL 커넥션 풀 고갈 장애

**1. 왜 MySQL 커넥션 풀이 고갈되었는가?**
   - Redis 장애로 캐시 조회 실패 시 DB 직접 조회가 급증했습니다.

**2. 왜 DB 직접 조회가 급증했는가?**
   - Redis 장애 시 fallback 로직이 모든 요청을 DB로 우회시켰습니다.

**3. 왜 모든 요청이 DB로 우회되었는가?**
   - Circuit Breaker 패턴이 적용되지 않아 장애 전파를 차단하지 못했습니다.

**4. 왜 Circuit Breaker가 적용되지 않았는가?**
   - 시스템 설계 시 장애 격리 및 회복탄력성(resilience) 패턴을 고려하지 않았습니다.

**5. 왜 회복탄력성 패턴을 고려하지 않았는가?**
   - 분산 시스템의 장애 전파 특성에 대한 이해와 경험이 부족했습니다.

**근본 원인**: 분산 시스템 장애 패턴에 대한 이해 부족 및 회복탄력성 설계 미흡

---

## 학습 사항 및 개선 방향

### 근본 원인 기반 개선 방향

**1. 운영 중심 설계 문화 정착**
- 기능 개발과 동시에 성능, 모니터링, 장애 대응 고려
- 설계 리뷰 시 운영 관점 체크리스트 필수 포함
- SRE(Site Reliability Engineering) 원칙 도입

**2. 현실적 성능 테스트 체계 구축**
- 운영 환경과 동일한 조건의 테스트 환경 구성
- 실제 데이터 크기, 네트워크 지연, 직렬화 오버헤드 반영
- 정기적 성능 회귀 테스트 자동화

**3. 회복탄력성 설계 패턴 적용**
- Circuit Breaker, Bulkhead, Timeout 패턴 표준화
- 장애 시뮬레이션 및 카오스 엔지니어링 도입
- 다중 계층 fallback 전략 수립

### 기술적 개선점
1. **Circuit Breaker 패턴**: 연쇄 장애 방지
2. **Bulkhead 패턴**: 리소스 격리로 장애 확산 차단
3. **백프레셔 제어**: 시스템 과부하 시 요청 제한

### 프로세스 개선점
1. **장애 드릴 실습**: 분기별 장애 대응 훈련
2. **카나리 배포**: 점진적 배포로 장애 위험 최소화
3. **카오스 엔지니어링**: 의도적 장애 주입으로 내성 강화

### 팀 역량 강화
1. **온콜 로테이션**: 장애 대응 경험 공유
2. **포스트모템 문화**: 비난 없는 학습 중심 회고
3. **장애 대응 매뉴얼**: 상황별 체크리스트 구비

---

**결론**: 현재 시스템은 안정적이나, 트래픽 3배 증가 시 Redis와 Kafka 병목이 예상됩니다. 사전 모니터링 체계와 표준 대응 절차로 장애 시 신속한 복구가 가능하도록 준비했습니다.